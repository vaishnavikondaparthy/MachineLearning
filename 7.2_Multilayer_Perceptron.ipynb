{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc7V8831YoO/3jNgmit6Rb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavikondaparthy/MachineLearning/blob/main/7.2_Multilayer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTILAYER PERCEPTRON\n"
      ],
      "metadata": {
        "id": "SdK2q3XYqWsN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qZJE0GriIutA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjp7tT6dJ8pp"
      },
      "source": [
        "- from sklearn.datasets import load_wine: This imports the load_wine function -from scikit-learn's datasets module, which allows us to load the Wine dataset.\n",
        "-from sklearn.model_selection import train_test_split: This imports the train_test_split function from scikit-learn's model_selection module, which helps us split the dataset into training and testing sets.\n",
        "-from sklearn.preprocessing import StandardScaler: This imports the StandardScaler class from scikit-learn's preprocessing module, which is used to standardize the dataset by removing the mean and scaling to unit variance.\n",
        "-import numpy as np: This imports the NumPy library, which provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k9l8EgOPIupJ"
      },
      "outputs": [],
      "source": [
        "# Load the wine dataset\n",
        "data = load_wine()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# print(data.head())"
      ],
      "metadata": {
        "id": "BekxlnYkLSWu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Sz7ZTw0aIulX"
      },
      "outputs": [],
      "source": [
        "#Splitting the dataset\n",
        "#This extracts the features (X) and target labels (y) from the loaded dataset.\n",
        "X, y = data.data, data.target\n",
        "# Split the dataset into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) #training set\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) #validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ3kheCoLPAY"
      },
      "source": [
        "-This splits the dataset into training and testing sets. Here, 30% of the data is reserved for testing, and random_state=42 sets a seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEC6YtPvOSPe",
        "outputId": "9194d2b3-5cfe-4008-d05a-997cf16c2178"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pMr46kABIuiS"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTm1p5bDLeBH"
      },
      "source": [
        "- scaler = StandardScaler(): This initializes a StandardScaler object.\n",
        "- X_train = scaler.fit_transform(X_train): This standardizes the training features using the fit_transform method, which computes the mean and standard deviation of each feature and then scales the features.\n",
        "- X_val = scaler.transform(X_val): This standardizes the validation features using the previously computed mean and standard deviation from the training set.\n",
        "- X_test = scaler.transform(X_test): This standardizes the test features using the same mean and standard deviation computed from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WPG81lqnIue3"
      },
      "outputs": [],
      "source": [
        "# Defining the model architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\", input_dim=X.shape[1]),  # Hidden layer with 64 neurons and ReLU activation\n",
        "    layers.Dense(32, activation=\"relu\"),                        # Another hidden layer with 32 neurons and ReLU activation\n",
        "    layers.Dense(3, activation=\"softmax\")                       # Output layer with softmax activation for multiclass classification\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVtO_dp4MBeI"
      },
      "source": [
        "- This defines a sequential model using Keras. We define three layers: two hidden layers with 64 and 32 neurons respectively, and an output layer with 3 neurons (one for each class in the dataset) using softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Labs0NFOIuby"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) #Adam is an optimization algorithm that adapts the learning rate during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fY5taW4MVqr"
      },
      "source": [
        "- This compiles the model. We specify the Adam optimizer, sparse categorical cross-entropy loss function (suitable for multiclass classification tasks), and accuracy as the evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOBn_NheIuYu",
        "outputId": "8b44c715-6ce3-49a2-a67d-d2003b27738e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 126ms/step - loss: 1.0655 - accuracy: 0.4516 - val_loss: 1.0014 - val_accuracy: 0.4815\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.9413 - accuracy: 0.6371 - val_loss: 0.9008 - val_accuracy: 0.7037\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.8340 - accuracy: 0.7500 - val_loss: 0.8112 - val_accuracy: 0.7778\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 0.7348 - accuracy: 0.8387 - val_loss: 0.7304 - val_accuracy: 0.7778\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.6428 - accuracy: 0.9113 - val_loss: 0.6556 - val_accuracy: 0.8148\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.5626 - accuracy: 0.9597 - val_loss: 0.5878 - val_accuracy: 0.8148\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.4886 - accuracy: 0.9597 - val_loss: 0.5257 - val_accuracy: 0.8148\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.4222 - accuracy: 0.9597 - val_loss: 0.4688 - val_accuracy: 0.8519\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.3634 - accuracy: 0.9677 - val_loss: 0.4179 - val_accuracy: 0.8519\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3137 - accuracy: 0.9758 - val_loss: 0.3738 - val_accuracy: 0.8519\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79c785cf5300>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnmJxwo0MlIf"
      },
      "source": [
        "- This trains the model on the training data for 10 epochs, using the validation data for validation during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irh7yit_IuU6",
        "outputId": "13aff77e-6d04-411a-d81b-9cfd436c0659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2510 - accuracy: 0.9630\n",
            "Test accuracy: 0.963\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfUUiBqKM9Kb"
      },
      "source": [
        "-  This evaluates the trained model on the test data and computes the test loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OpL0BXLIuRI",
        "outputId": "873649e9-7b15-456b-86eb-77c8300939a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 141ms/step\n",
            "Predictions:\n",
            "Sample 1: [1. 0. 0.]\n",
            "Sample 2: [1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on new data\n",
        "X_new = np.array([\n",
        "    [13.71, 1.86, 2.36, 16.6, 101.2, 2.61, 2.88, 0.27, 1.69, 3.8, 1.11, 4.0, 1035],\n",
        "    [13.56, 1.73, 2.46, 20.5, 116.0, 2.96, 2.78, 0.2, 2.45, 6.25, 0.98, 3.03, 1120]\n",
        "])\n",
        "predictions = model.predict(X_new)\n",
        "print(\"Predictions:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Sample {i+1}: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNHldOiHNLFO"
      },
      "source": [
        "- predictions = model.predict(X_new): This makes predictions on new data (X_new) using the trained model.\n",
        "The print(\"Predictions:\") line prints a header for the predictions.\n",
        "for i, pred in enumerate(predictions): print(f\"Sample {i+1}: {pred}\"): This iterates over the predictions and prints the predicted probabilities for each sample in X_new."
      ]
    }
  ]
}